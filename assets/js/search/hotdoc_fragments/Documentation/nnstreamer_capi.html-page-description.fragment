fragment_downloaded_cb({"url":"Documentation/nnstreamer_capi.html#page-description","fragment":"You can easily create and efficiently execute data stream pipelines that consist of neural networks as filters in pipelines.\nThe main features of the Machine Learning Inference API include:\nConstruction of data pipeline based on GStreamer\nYou can compose the data stream pipeline through Machine Learning Inference with various elements of GStreamer and NNStreamer.\nSingle API and Pipeline API\nThere are two types of Machine Learning Inference API - Single API and Pipeline API.\nSingle API is useful for a simple usage scenario of neural network models. It allows invoking a neural network model with a single instance of input data for the model directly. It is useful if you have the input data pre-processed with the application itself and there are no complex interactions between neural network models, data processors, or data stream paths.\nPipeline API allows developers to construct and execute pipelines with multiple neural network models, multiple inputs and output nodes, multiple data processors, pre-and-post processors, and various data path manipulators. Besides, if the input is online data or streamed data, Pipeline API simplifies your application and improves its performance.\nSupport various neural network frameworks (NNFW)\nTensorFlow, TensorFlow-Lite, Caffe2, and PyTorch are the supported neural network frameworks. Neural network model files trained by such frameworks can be imported as filters of pipelines directly.\nCustom filters, which are neural network models implemented directly with programming languages including C/C++ and Python, maybe imported as filters of pipelines directly as well.\nNote\nThe devices powered by Tizen OS can contain TensorFlow-Lite only. Ensure that the neural network frameworks that you want to use are installed.\nTo enable your application to use the machine learning functionality:\nTo use the functions and data types of the Machine Learning Inference API, include the <nnstreamer.h> header file in your application:\nTo use the Machine Learning Inference API, include the following features in your tizen-manifest.xml file:\nThis section shows how to load a model without the construction of pipelines.\nOpen a model file:\nTo load a model file, two ml_tensors_info_h are required. in_info contains the information of the input tensors, and out_info contains the information of the output tensors. For more information, see Tensors Information.\nGet the Tensors Information.\nAfter opening the model, use the following functions to bring the information of the input and output tensors:\nInvoke the model with input and output Tensors Data.\nThe model can be invoked with input and output tensors data. The result is included in the output tensors data:\nClose the opened handle:\nThis section shows how to create a pipeline.\nConstruct a pipeline with the GStreamer elements.\nDifferent pipelines can be constructed using various GStreamer elements:\nStart the pipeline and get state:\nStop the pipeline and get state:\nDestroy the pipeline.\nWhen no longer needed, destroy the pipeline:\nYou need to manipulate the input and the output data to run neural network models with Machine Learning Inference API. In addition, you can construct pipelines that can be controlled.\nFollowings are the available elements:\nSource\nThe configuration of the data source element is required to set the input tensor data:\nml_pipeline_src_get_handle() controls the appsrc element with the name srcx:\nYou can check the information of input tensors using srchandle:\nThe input tensor data can be filled according to the info:\nAfter using the data source element, release the handle:\nSink\nThe configuration of the data sink element is required to get the output tensor data:\nappsink element with the name sinkx becomes reachable through ml_pipeline_sink_register():\nYou can get the data from sink_callback(), whenever appsink named sinkx receives data:\nRelease the sinkhandle through ml_pipeline_sink_unregister():\nValve\nThis element is used to control the stream of a pipeline:\nBy default, valve named valve1 of the pipeline is opened. You can control the valve using ml_pipeline_valve_h:\nAfter you start a pipeline, you can control the stream of the pipeline with a valve:\nYou can also open the pipeline by controlling the stream of a pipeline with a valve:\nBefore you destroy the pipeline, release ml_pipeline_valve_h:\nSwitch\nThe switch element is used when you need only one working branch from a pipeline that has multiple branches:\n\nGet ml_pipeline_switch_h. The name of the switch in this pipeline is ins:\nYou can control the switch using the handle ml_pipeline_switch_h:\nBefore you destroy the pipeline, release ml_pipeline_switch_h:\nThe following image shows the switch at the end of the pipeline:\n\nFor more information about the pipeline states, see GStreamer guide.\nml_tensors_info_h contains the information of tensors. The tensor info can be managed using the following functions:\nCreate and destroy\nSet functions\nGet functions\nml_tensors_data_h contains the raw data of tensors. The tensor data can be managed using the following functions:\nCreate and destroy\nGet and set tensor data\n\n\nConstruction of data pipeline based on GStreamer\nYou can compose the data stream pipeline through Machine Learning Inference with various elements of GStreamer and NNStreamer.\n\n\nSingle API and Pipeline API\nThere are two types of Machine Learning Inference API - Single API and Pipeline API.\nSingle API is useful for a simple usage scenario of neural network models. It allows invoking a neural network model with a single instance of input data for the model directly. It is useful if you have the input data pre-processed with the application itself and there are no complex interactions between neural network models, data processors, or data stream paths.\nPipeline API allows developers to construct and execute pipelines with multiple neural network models, multiple inputs and output nodes, multiple data processors, pre-and-post processors, and various data path manipulators. Besides, if the input is online data or streamed data, Pipeline API simplifies your application and improves its performance.\n\n\nSupport various neural network frameworks (NNFW)\nTensorFlow, TensorFlow-Lite, Caffe2, and PyTorch are the supported neural network frameworks. Neural network model files trained by such frameworks can be imported as filters of pipelines directly.\nCustom filters, which are neural network models implemented directly with programming languages including C/C++ and Python, maybe imported as filters of pipelines directly as well.\n\nNote\nThe devices powered by Tizen OS can contain TensorFlow-Lite only. Ensure that the neural network frameworks that you want to use are installed.\n\n\n\n\n\nSource\nThe configuration of the data source element is required to set the input tensor data:\nchar pipeline[] = \"appsrc name=srcx ! other/tensor,dimension=(string)4:1:1:1,type=(string)uint8,framerate=(fraction)0/1 ! tensor_sink\";\n\nml_pipeline_src_get_handle() controls the appsrc element with the name srcx:\nml_pipeline_h handle;\nml_pipeline_src_h srchandle;\n\nstatus = ml_pipeline_construct (pipeline, NULL, NULL, &handle);\nstatus = ml_pipeline_start (handle);\nstatus = ml_pipeline_src_get_handle (handle, \"srcx\", &srchandle);\n\nYou can check the information of input tensors using srchandle:\nml_tensors_info_h info;\n\nstatus = ml_pipeline_src_get_tensors_info (srchandle, &info);\n\nThe input tensor data can be filled according to the info:\nml_tensors_data_h data;\n\nstatus = ml_tensors_data_create (info, &data);\n\nfor (i = 0; i < 10; i++) {\n  uintarray1[i] = (uint8_t *) malloc (4);\n  uintarray1[i][0] = i + 4;\n  uintarray1[i][1] = i + 1;\n  uintarray1[i][2] = i + 3;\n  uintarray1[i][3] = i + 2;\n}\n\nstatus = ml_tensors_data_set_tensor_data (data, 0, uintarray1[0], 4);\n\n/* Setting the policy of raw data pointer */\nstatus = ml_pipeline_src_input_data (srchandle, data, ML_PIPELINE_BUF_POLICY_DO_NOT_FREE);\n\nAfter using the data source element, release the handle:\nstatus = ml_pipeline_src_release_handle (srchandle);\n\n\n\nSink\nThe configuration of the data sink element is required to get the output tensor data:\nchar pipeline[] = \"videotestsrc num-buffers=3 ! videoconvert ! tensor_converter ! appsink name=sinkx sync=false\";\n\nappsink element with the name sinkx becomes reachable through ml_pipeline_sink_register():\nint status;\nml_pipeline_h handle;\nml_pipeline_sink_h sinkhandle;\n\nstatus = ml_pipeline_sink_register (handle, \"sinkx\", sink_callback, user_data, &sinkhandle);\n\nYou can get the data from sink_callback(), whenever appsink named sinkx receives data:\ntypedef void (*ml_pipeline_sink_cb) (const ml_tensors_data_h data, const ml_tensors_info_h info, void *user_data);\n\nRelease the sinkhandle through ml_pipeline_sink_unregister():\nstatus = ml_pipeline_sink_unregister (sinkhandle);\n\n\n\nValve\nThis element is used to control the stream of a pipeline:\nchar pipeline[] = \"videotestsrc is-live=true ! videoconvert ! videoscale ! video/x-raw,format=RGBx,width=16,height=16,framerate=10/1 ! tensor_converter ! valve name=valve1 ! fakesink\";\nint status = ml_pipeline_construct (pipeline, NULL, NULL, &handle);\n\nBy default, valve named valve1 of the pipeline is opened. You can control the valve using ml_pipeline_valve_h:\nml_pipeline_h handle;\nml_pipeline_valve_h valve1;\n\nstatus = ml_pipeline_valve_get_handle (handle, \"valve1\", &valve1);\n\nAfter you start a pipeline, you can control the stream of the pipeline with a valve:\nstatus = ml_pipeline_start (handle);\n\nstatus = ml_pipeline_valve_set_open (valve1, false); /* Close */\n\nYou can also open the pipeline by controlling the stream of a pipeline with a valve:\nstatus = ml_pipeline_valve_set_open (valve1, true); /* Open */\n\nBefore you destroy the pipeline, release ml_pipeline_valve_h:\nstatus = ml_pipeline_valve_release_handle (valve1); /* Release valve handle */\n\n\n\nSwitch\nThe switch element is used when you need only one working branch from a pipeline that has multiple branches:\n\nchar pipeline[] = \"input-selector name=ins ! tensor_converter ! tensor_sink name=sinkx videotestsrc is-live=true ! videoconvert ! ins.sink_0 videotestsrc num-buffers=3 is-live=true ! videoconvert ! ins.sink_1\";\n\nGet ml_pipeline_switch_h. The name of the switch in this pipeline is ins:\nml_pipeline_h handle;\nml_pipeline_switch_h switchhandle;\nml_pipeline_switch_e type;\n\nstatus = ml_pipeline_construct (pipeline, NULL, NULL, &handle);\nstatus = ml_pipeline_switch_get_handle (handle, \"ins\", &type, &switchhandle);\n\nYou can control the switch using the handle ml_pipeline_switch_h:\nstatus = ml_pipeline_switch_select (switchhandle, \"sink_1\");\n\nBefore you destroy the pipeline, release ml_pipeline_switch_h:\nstatus = ml_pipeline_switch_release_handle (switchhandle);\n\nThe following image shows the switch at the end of the pipeline:\n\nchar pipeline[] = \"videotestsrc is-live=true ! videoconvert ! tensor_converter ! output-selector name=outs outs.src_0 ! tensor_sink name=sink0 async=false outs.src_1 ! tensor_sink name=sink1 async=false\"\n\n\n\n\n\nCreate and destroy\nml_tensors_info_h info;\nstatus = ml_tensors_info_create (&info);\nstatus = ml_tensors_info_destroy (info);\n\n\n\nSet functions\n/* Set how many tensors exist */\nstatus = ml_tensors_info_set_count (info, 1);\n\n/* Set the type of the tensor_0 as UINT8 */\nstatus = ml_tensors_info_set_tensor_type (info, 0, ML_TENSOR_TYPE_UINT8);\n\n/* Set the dimension of the tensor_0 as in_dim */\nstatus = ml_tensors_info_set_tensor_dimension (info, 0, in_dim);\n\n/* Set the name of the tensor_0 as \"tensor-name-test\" */\nstatus = ml_tensors_info_set_tensor_name (info, 0, \"tensor-name-test\");\n\n\n\nGet functions\n/* Get how many tensors exist */\nstatus = ml_tensors_info_get_count (info, &num);\n\n/* Get the type of the tensor_0 */\nstatus = ml_tensors_info_get_tensor_type (info, 0, &out_type);\n\n/* Get the dimension of the tensor_0 */\nstatus = ml_tensors_info_get_tensor_dimension (info, 0, in_dim);\n\n/* Get the name of the tensor_0 */\nstatus = ml_tensors_info_get_tensor_name (info, 0, &out_name);\n\n/* Get the size of the tensor_0 */\nstatus = ml_tensors_info_get_tensor_size (info, 0, &data_size);\n\n\n\n\n\nCreate and destroy\nml_tensors_data_h data;\nml_tensors_info_h info;\n\nstatus = ml_tensors_data_create (info, &data);\nstatus = ml_tensors_data_destroy (data);\n\n\n\nGet and set tensor data\n/* Get tensor data */\nvoid *data_ptr;\nsize_t data_size;\nstatus = ml_tensors_data_get_tensor_data (data, 0, &data_ptr, &data_size);\n\n/* Set tensor data */\nuint8_t dummy[4] = {1, 1, 1, 1};\nstatus = ml_tensors_data_set_tensor_data (data, 0, dummy, 1);\n\n\n\n\nDependencies\n\nTizen 5.5 and Higher for Mobile\nTizen 5.5 and Higher for Wearable\n\n\n\n\nTizen 5.5 and Higher for Mobile\nTizen 5.5 and Higher for Wearable\n\n"});
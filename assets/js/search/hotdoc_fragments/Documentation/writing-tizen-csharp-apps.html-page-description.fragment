fragment_downloaded_cb({"url":"Documentation/writing-tizen-csharp-apps.html#page-description","fragment":"This document provides you how to write a Tizen .NET Application with Machine Learning APIs.\nSince Tizen 5.5, Machine Learning Inference functionality has been provided on Mobile, Wearable and TV profile.\nIn order to use this functionality, you need to install Visual Studio Tools for Tizen and Tizen SDK. You can find the detailed guide for this at the below link.\nMachine learning (ML) inference feature introduces how you can easily invoke the neural network model and get the inference output result effortlessly and efficiently.\nYou can use the following machine learning feature in your .NET applications:\nYou can use the Tizen.MachineLearning.Inference.SingleShot class, to load the existing neural network model or your own specific model from the storage. After loading the model, you can invoke it with a single instance of input data. Then, you can get the inference output result.\nYou can also use the Pipeline feature to manage the topology of data and the interconnection between processors and models. This feature is available in Native APIs from Tizen 5.5. However, this feature is not available in .NET APIs. This feature will be available in the .NET APIs from the next Tizen version.\nThe main features of the Tizen.MachineLearning.Inference are:\nManaging tensor information, which is the metadata: dimensions and types of tensors\nYou can configure the input and output Tensor Information such as its name, data type and dimension.\nLoading a neural network model and configuring a runtime environment\nYou can load the neural network model from storage and configure a runtime environment.\nInvoking the neural network model with input data\nAfter setting up the SingleShot instance with its required information, you can invoke the model with the input data and get the inference output result.\nFetching the inference result after invoking\nYou can fetch the inference result after invoking the respective model.\nTo enable your application to use the Machine Learning Inference API functionality:\nTo use the methods and properties of the Tizen.MachineLearning.Inference.SingleShot class or its related classes such as Tizen.MachineLearning.Inference.TensorsData and Tizen.MachineLearning.Inference.TensorsInfo, include the Tizen.MachineLearning.Inference namespace in your application:\nIf the model file you want to use is located in the media storage or the external storage, the application has to request permission by adding the following privileges to the tizen-manifest.xml file:\n\nIn the example mentioned in this page, the MobileNet v1 model for TensorFlow Lite is used. This model is used for image classification. The input data type of the model is specified as bit width of each Tensor and its input dimension is 3 X 224 X 224. The output data type of the model is the same as the input datatype but the output dimension is 1001 X 1 X 1 X 1.\nTo configure the tensor information, you need to create a new instance of the Tizen.MachineLearning.Inference.TensorsInfo class. Then, you can add the tensor information such as datatype, dimension, and name (optional) as shown in the following code:\n\nSince the model file is located in the resource directory of your own application, you need to get its absolute path:\nYou can load the neural network model from storage and configure a runtime environment with the Tizen.MachineLearning.Inference.SingleShot class. The first parameter is the absolute path to the neural network model file. The remaining two parameters are the input and the output TensorsInfo instances. If there is an invalid parameter, ArgumentException is raised:\n\nTo invoke the neural network model, you need to create the Tizen.MachineLearning.Inference.TensorsData instance to pass the input data of the model. You can add various types of tensor data, which are already specified in the TensorInfo instance. However, the maximum size of TensorsData is 16. If the limit is exceeded, then IndexOutOfRangeException is raised. Input data is passed in a byte array format, byte[]:\nAfter preparing the input data, you can invoke the model and get the inference output result. The SingleShot.Invoke() method gets the input data to be inferred as a parameter and returns the Tizen.MachineLearning.InferenceTensorsData instance, which contains the inference result:\n\nAfter calling the Invoke() method of the Tizen.MachineLearning.Inference.SingleShot class,\nthe Tizen.MachineLearning.Inference.TensorsData instance is returned as the inference result.\nThe result can have multiple output data. Therefore, you have to fetch each data using the GetTensorData() method. If the limit is exceeded, then IndexOutOfRangeException is raised:\nThe TensorsData class is used to send the input data to a neural network model. In addition, it provides the Count property to get the number of tensors:\n\nhttps://docs.tizen.org/application/vstools/install\n\n\n\nManaging tensor information, which is the metadata: dimensions and types of tensors\nYou can configure the input and output Tensor Information such as its name, data type and dimension.\n\n\nLoading a neural network model and configuring a runtime environment\nYou can load the neural network model from storage and configure a runtime environment.\n\n\nInvoking the neural network model with input data\nAfter setting up the SingleShot instance with its required information, you can invoke the model with the input data and get the inference output result.\n\n\nFetching the inference result after invoking\nYou can fetch the inference result after invoking the respective model.\n\n\n\nDependencies\n\nTizen 5.5 and Higher\n\n\n\n\nTizen 5.5 and Higher\n\n"});
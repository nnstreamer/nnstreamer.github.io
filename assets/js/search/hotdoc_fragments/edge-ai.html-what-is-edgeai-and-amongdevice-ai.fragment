fragment_downloaded_cb({"url":"edge-ai.html#what-is-edgeai-and-amongdevice-ai","fragment":"What is Edge-AI and Among-Device AI?\nWith on-device AI, different embedded devices including mobile phones, TVs, refrigerators, vacuum cleaners, and all the other sorts of deviecs have started running deep neural network models.\nUnlike conventional AI services running in cloud servers, on-device AI services are often limited by the limited computing resources of the given devices; they usually have limited size of memory, limited processing power, or limited energy, which varies per device.\nMoreover, such devices have different availability of data; each device may have different sensors or different sensing targets (e.g., view angles of cameras), which might be beneficial if a device has access to others.\nAmong-Device AI (a.k.a. Edge-AI in some industry context) tries to mitigate such issues by connecting such devices and let them share data and processing power between them.\nIn other words, Among-Device AI / Edge-AI allows distributed computing for AI algorithms, which can distribute computing workloads and share data from different devices.\nRunning AI at the ‘edge’ of the local network removes the requirement for the device to be connected to the internet or centralized servers like the cloud.\nEdge AI offers significant improvements as far as response speeds and data security.\nExecuting AI close to the data source allows for processes like data creation and decision-making to take place in milliseconds, making Edge AI ideal for applications where near-instantaneous responses are essential.\nAmong-Device AI / Edge-AI can be often extended to conventional cloud AI services so that parts of data can be sent to clouds for further processing or data gathering for future neural network models.\nTechnically, for pipeline frameworks such as NNStreamer and GStreamer, they are not very different from among-device AI between embedded devices (exactly same code can be applied!).\nIn the NNStreamer's point of view, among-device AI can be achieved by connecting NNStreamer pipelines running in different devices.\nBy connecting pipelines of different devices, devices can send data for inferences to other devices and receive inference results or data from sensors of other devices.\nMoreover, by completing #3745, this can be expanded to federated learning and training offloading, which is expected to be enabled by 2023.\n\nReference\n\n\nOn the Edge - How Edge AI is reshaping the future, Samsung Newsroom\n\nToward Among-Device AI from On-Device AI with Stream Pipelines, ICSE 2022 SEIP\n\n\n\n\n\nOn the Edge - How Edge AI is reshaping the future, Samsung Newsroom\n\nToward Among-Device AI from On-Device AI with Stream Pipelines, ICSE 2022 SEIP\n\n"});
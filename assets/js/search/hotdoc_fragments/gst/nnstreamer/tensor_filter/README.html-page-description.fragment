fragment_downloaded_cb({"url":"gst/nnstreamer/tensor_filter/README.html#page-description","fragment":"This is the main element of the whole NNStreamer project. This connects gstreamer data stream with neural network frameworks (NNFW) such as Tensorflow or Caffe.\ntensor_filter is supposed to attach an instance of neural network model with the given NNFW as a filter to a gstreamer stream.\nThe input/output stream data type is either other/tensor or other/tensors.\nFor example, if the output of a neural network model has two distinctive output tensors, \"Bounding boxes, uint32[100][4]\" and \"Labels, uint32[100]\", you may use other/tensors as the source pad capability to have both tensors in a single frame of the source pad.\nNote that tensor_filter supports one always source pad and one always sink pad.\nYou should specify neural network framework, model path, and model meta information.\nMore launch line examples here: nnstreamer example\n*note: If in/out type and dimension can get from the model such as tensorflow-lite, you do not need to specify the properties.\nOne always sink pad exists. The capability of sink pad is other/tensor and other/tensors.\nThe number of frames in a buffer is always 1. Although the data semantics of a tensor may have multiple distinct data frames in a single tensor.\nOne always source pad exists. The capability of source pad is other/tensor and other/tensors.\nThe number of frames in a buffer is always 1. Although the data semantics of a tensor may have multiple distinct data frames in a single tensor.\nIn a nnstreamer pipeline, the QoS is currently satisfied by adjusting input or output framerate, initiated by 'tensor_rate' element.\nWhen 'tensor_filter' receives a throttling QoS event from the 'tensor_rate' element, it compares the average processing latency and throttling delay, and takes the maximum value as the threshold to drop incoming frames by checking a buffer timestamp.\nIn this way, 'tensor filter' can avoid unnecessary calculation and adjust a framerate, effectively reducing resource utilizations.\nEven in the case of receiving QoS events from multiple downstream pipelines (e.g., tee), 'tensor_filter' takes the minimum value as the throttling delay for downstream pipeline with more tight QoS requirement. Lastly, 'tensor_filter' also sends QoS events to upstream elements (e.g., tensor_converter, tensor_src) to possibly reduce incoming framerates, which is a better solution than dropping framerates.\nSelect the input tensor(s) to invoke the models\nIf the input is tensors '0,1,2', only tensors '0' and '2' are used to invoke the model\nSelect the output tensor(s) from the input tensor(s) and/or model output\nSuppose the model receives tensors '0,1' as an input and outputs tensor '0,1,2'.\nSrc pad of the tensor_filter can produce input tensor '0' and output tensors '0,2' using output-combination.\nThis is the main placeholder for all different subcomponents. With the property, FRAMEWORK, this main component loads the proper subcomponent (e.g., tensorflow-lite support, custom support, or other addtional NNFW supports).\nThe main component is supposed process the standard properties for subcomponents as well as processing the input/output dimensions.\nThe subcomponents as supposed to fill in GstTensor_Filter_Framework struct and register it with supported array in tensor_filter.h.\nNote that the registering sturcture may be updated later. (We may follow what Linux.kernel/drivers/devfreq/devfreq.c does)\nThis should fill in GstTensor_Filter_Framework supporting tensorflow_lite.\nNeural network and streameline developers may define their own tensor postprocessing operations with tensor_filter_custom.\nWith nnstreamer-devel package installed at build time (e.g., BuildRequires: pkgconfig(nnstreamer) in .spec file), develerops can implement their own functions and expose their functions via NNStreamer_custom_class defined in tensor_fitler_custom.h.\nThe resulting custom developer plugin should exist as a shared library (.so) with the symbol NNStreamer_custom exposed with all the func defined in NNStreamer_custom_class.\n\nWith other/tensor, you may push (or pull) a single tensor for an instance of inference for the given model.\nWith other/tensors, you may push (or pull) multiple tensors for an instance of inference for the given model.\n\n\nMulti-tensor\nCustom filters\nFramerate policies\nRecurrent network support\nSupported framework\n\nTensorFlow, TensorFlow-lite, NNFW(ONE), Caffe2, Python 3, PyTorch, OpenVINO, EdgeTPU, ArmNN, TensorRT, SNPE, SNAP\n\n\n\n\nTensorFlow, TensorFlow-lite, NNFW(ONE), Caffe2, Python 3, PyTorch, OpenVINO, EdgeTPU, ArmNN, TensorRT, SNPE, SNAP\n\n\nTimestamp handling\n\n\nNo known bugs except for NYI items\n\n\nWe do not support in-place operations with tensor_filter. Actually, with tensor_filter, in-place operations are considered harmful for the performance and correctness.\nIt is supposed that there is no memcpy from the previous element's source pad to this element's sink or from this element's source to the next element's sink pad.\n\n"});
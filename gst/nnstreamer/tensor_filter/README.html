<!DOCTYPE html>
<html lang="en">
<head>

<base href="../../..">

<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>tensor_filter</title>

<link rel="stylesheet" href="assets/css/custom_bootstrap.css" type="text/css">
<link rel="stylesheet" href="assets/css/bootstrap-toc.min.css" type="text/css">
<link rel="stylesheet" href="assets/css/frontend.css" type="text/css">
<link rel="stylesheet" href="assets/css/jquery.mCustomScrollbar.min.css">
<link rel="stylesheet" href="assets/js/search/enable_search.css" type="text/css">

<link rel="stylesheet" href="assets/css/extra_frontend.css" type="text/css">
<link rel="stylesheet" href="assets/css/prism-tomorrow.css" type="text/css">

<script src="assets/js/mustache.min.js"></script>
<script src="assets/js/jquery.js"></script>
<script src="assets/js/bootstrap.js"></script>
<script src="assets/js/scrollspy.js"></script>
<script src="assets/js/typeahead.jquery.min.js"></script>
<script src="assets/js/search.js"></script>
<script src="assets/js/compare-versions.js"></script>
<script src="assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
<script src="assets/js/bootstrap-toc.min.js"></script>
<script src="assets/js/jquery.touchSwipe.min.js"></script>
<script src="assets/js/anchor.min.js"></script>
<script src="assets/js/tag_filtering.js"></script>
<script src="assets/js/language_switching.js"></script>

<script src="assets/js/lines_around_headings.js"></script>

<script src="assets/js/trie.js"></script>
<script src="assets/js/prism-core.js"></script>
<script src="assets/js/prism-autoloader.js"></script>
<script src="assets/js/prism_autoloader_path_override.js"></script>

<link rel="icon" type="image/png" href="assets/images/nnstreamer_logo.png">

</head>

<body class="no-script
">

<script>
$('body').removeClass('no-script');
</script>

<nav class="navbar navbar-fixed-top navbar-default" id="topnav">
	<div class="container-fluid">
		<div class="navbar-right">
			<a id="toc-toggle">
				<span class="glyphicon glyphicon-menu-right"></span>
				<span class="glyphicon glyphicon-menu-left"></span>
			</a>
			<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-wrapper" aria-expanded="false">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			<form class="navbar-form pull-right" id="navbar-search-form">
                               <div class="form-group has-feedback">
                                       <input type="text" class="form-control input-sm" name="search" id="sidenav-lookup-field" placeholder="search" disabled>
				       <span class="glyphicon glyphicon-search form-control-feedback" id="search-mgn-glass"></span>
                               </div>
                        </form>
		</div>
		<div class="navbar-header">
			<a id="sidenav-toggle">
				<span class="glyphicon glyphicon-menu-right"></span>
				<span class="glyphicon glyphicon-menu-left"></span>
			</a>
			<a id="home-link" href="index.html" class="hotdoc-navbar-brand">
				<img src="assets/images/nnstreamer_logo.png" alt="Home">
			</a>
		</div>
		<div class="navbar-collapse collapse" id="navbar-wrapper">
			<ul class="nav navbar-nav" id="menu">
				
<li class="dropdown">
    <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
        API References<span class="caret"></span>
    </a>
	<ul class="dropdown-menu" id="modules-menu">
					<li>
				<a href="doc-index.html">NNStreamer doc</a>
			</li>
					<li>
				<a href="gst/nnstreamer/README.html">NNStreamer Elements</a>
			</li>
					<li>
				<a href="nnstreamer-example/index.html">NNStreamer Examples</a>
			</li>
					<li>
				<a href="API-reference.html">API reference</a>
			</li>
		</ul>
</li>

<li>
	<a href="doc-index.html">Documents</a>
</li>
<li>
	<a href="gst/nnstreamer/README.html">Elements</a>
</li>
<li>
	<a href="tutorials.html">Tutorials</a>
</li>
<li>
	<a href="API-reference.html">API reference</a>
</li>

			</ul>
			<div class="hidden-xs hidden-sm navbar-text navbar-center">
							</div>
		</div>
	</div>
</nav>

<main>
<div data-extension="core" data-hotdoc-in-toplevel="True" data-hotdoc-project="NNStreamer" data-hotdoc-ref="gst/nnstreamer/tensor_filter/README.html" class="page_container" id="page-wrapper">
<script src="assets/js/utils.js"></script>

<div class="panel panel-collapse oc-collapsed" id="sidenav" data-hotdoc-role="navigation">
	<script src="assets/js/navigation.js"></script>
	<script src="assets/js/sitemap.js"></script>
</div>

<div id="body">
	<div id="main">
				    <div id="page-description" data-hotdoc-role="main">
        <h1 id="nnstreamertensor_filter">NNStreamer::tensor_filter</h1>
<p>This is the main element of the whole NNStreamer project. This connects gstreamer data stream with neural network frameworks (NNFW) such as Tensorflow or Caffe.<br>
<code>tensor_filter</code> is supposed to attach an instance of neural network model with the given NNFW as a filter to a gstreamer stream.<br>
The input/output stream data type is either <code>other/tensor</code> or <code>other/tensors</code>.</p>
<ul>
<li>With <code>other/tensor</code>, you may push (or pull) a single tensor for an instance of inference for the given model.</li>
<li>With <code>other/tensors</code>, you may push (or pull) multiple tensors for an instance of inference for the given model.</li>
</ul>
<p>For example, if the output of a neural network model has two distinctive output tensors, "Bounding boxes, uint32[100][4]" and "Labels, uint32[100]", you may use <code>other/tensors</code> as the source pad capability to have both tensors in a single frame of the source pad.<br>
Note that <code>tensor_filter</code> supports one always source pad and one always sink pad.</p>
<h2 id="example-launch-line-tensorflow">Example launch line (tensorflow)</h2>
<pre><code>... input other/tensor(s) ! \
    tensor_filter framework=tensorflow model=${PATH_TO_MODEL} \
                  input=784:1 inputtype=float32 inputname=input output=10:1 \
                  outputtype=float32 outputname=softmax ! \
    output other/tensor(s) ! ...
</code></pre>
<p>You should specify neural network framework, model path, and model meta information.<br>
More launch line examples here: <a href="https://github.com/nnstreamer/nnstreamer-example/tree/master/bash_script">nnstreamer example</a><br>
*note: If in/out type and dimension can get from the model such as tensorflow-lite, you do not need to specify the properties.</p>
<h2 id="supported-features">Supported features</h2>
<ul>
<li>Multi-tensor</li>
<li>Custom filters</li>
<li>Framerate policies</li>
<li>Recurrent network support</li>
<li>Supported framework
<ul>
<li>TensorFlow, TensorFlow-lite, NNFW(ONE), Caffe2, Python 3, PyTorch, OpenVINO, EdgeTPU, ArmNN, TensorRT, SNPE, SNAP</li>
</ul>
</li>
</ul>
<h2 id="planned-features">Planned Features</h2>
<ul>
<li>Timestamp handling</li>
</ul>
<h2 id="known-bugs-or-concerns">Known Bugs or Concerns</h2>
<ul>
<li>No known bugs except for NYI items</li>
</ul>
<h2 id="sink-pads">Sink Pads</h2>
<p>One always sink pad exists. The capability of sink pad is <code>other/tensor</code> and <code>other/tensors</code>.<br>
The number of frames in a buffer is always 1. Although the data semantics of a tensor may have multiple distinct data frames in a single tensor.</p>
<h2 id="source-pads">Source Pads</h2>
<p>One always source pad exists. The capability of source pad is <code>other/tensor</code> and <code>other/tensors</code>.<br>
The number of frames in a buffer is always 1. Although the data semantics of a tensor may have multiple distinct data frames in a single tensor.</p>
<h2 id="performance-characteristics">Performance Characteristics</h2>
<ul>
<li>We do not support in-place operations with tensor_filter. Actually, with tensor_filter, in-place operations are considered harmful for the performance and correctness.</li>
<li>It is supposed that there is no memcpy from the previous element's source pad to this element's sink or from this element's source to the next element's sink pad.</li>
</ul>
<h2 id="qos-policy">QoS policy</h2>
<p>In a nnstreamer pipeline, the QoS is currently satisfied by adjusting input or output framerate, initiated by 'tensor_rate' element.<br>
When 'tensor_filter' receives a throttling QoS event from the 'tensor_rate' element, it compares the average processing latency and throttling delay, and takes the maximum value as the threshold to drop incoming frames by checking a buffer timestamp.<br>
In this way, 'tensor filter' can avoid unnecessary calculation and adjust a framerate, effectively reducing resource utilizations.<br>
Even in the case of receiving QoS events from multiple downstream pipelines (e.g., tee), 'tensor_filter' takes the minimum value as the throttling delay for downstream pipeline with more tight QoS requirement. Lastly, 'tensor_filter' also sends QoS events to upstream elements (e.g., tensor_converter, tensor_src) to possibly reduce incoming framerates, which is a better solution than dropping framerates.</p>
<h2 id="inout-combination">In/Out combination</h2>
<h3 id="input-combination">Input combination</h3>
<p>Select the input tensor(s) to invoke the models</p>
<h4 id="example-launch-line">Example launch line</h4>
<pre><code>... (tensors 0,1,2) ! tensor_filter framework=auto model=${MODEL_PATH} input-combination=0,2 ! (output tensor(s) stream) ...
</code></pre>
<p>If the input is tensors '0,1,2', only tensors '0' and '2' are used to invoke the model</p>
<h3 id="output-combination">Output combination</h3>
<p>Select the output tensor(s) from the input tensor(s) and/or model output</p>
<h4 id="example-launch-line1">Example launch line</h4>
<pre><code>... (tensors 0,1) ! tensor_filter framework=auto model=${MODEL_PATH} output-combination=i0,o0,o2 ! (input tensor 0 and output tensor 0 and 2) ...
</code></pre>
<p>Suppose the model receives tensors '0,1' as an input and outputs tensor '0,1,2'.<br>
Src pad of the tensor_filter can produce input tensor '0' and output tensors '0,2' using output-combination.</p>
<h3 id="comparison-of-tee-and-combination-option">Comparison of tee and combination option</h3>
<h4 id="object-detection-using-tee">Object detection using tee</h4>
<p>The video is the same as the original camera output and the labels and bounding boxes are updated after processing in the tensor filter.</p>
<ul>
<li>launch script</li>
</ul>
<pre><code>gst-launch-1.0 \
v4l2src name=cam_src ! videoscale ! videoconvert ! video/x-raw,width=640,height=480,format=RGB,framerate=30/1 ! tee name=t \
  t. ! queue leaky=2 max-size-buffers=2 ! videoscale ! tensor_converter ! \
    tensor_filter framework=tensorflow model=tf_model/ssdlite_mobilenet_v2.pb \
      input=3:640:480:1 inputname=image_tensor inputtype=uint8 \
      output=1:1:1:1,100:1:1:1,100:1:1:1,4:100:1:1 \
      outputname=num_detections,detection_classes,detection_scores,detection_boxes \
      outputtype=float32,float32,float32,float32 ! \
    tensor_decoder mode=bounding_boxes option1=mobilenet-ssd-postprocess option2=tf_model/coco_labels_list.txt option4=640:480 option5=640:480 ! \
    compositor name=mix sink_0::zorder=2 sink_1::zorder=1 ! videoconvert ! ximagesink \
  t. ! queue leaky=2 max-size-buffers=10 ! mix.
</code></pre>
<ul>
<li>Graphical description of the pipeline
<img src="gst/nnstreamer/tensor_filter/./filter_tee.png" alt="tee-pipeline-img" id="teepipelineimg">
</li>
</ul>
<h4 id="object-detection-using-output-combination-option">Object detection using output combination option</h4>
<p>The original video frame is passed to output of tensor-filter using the property output-combination.</p>
<ul>
<li>launch script</li>
</ul>
<pre><code>gst-launch-1.0 \
v4l2src name=cam_src ! videoscale ! videoconvert ! video/x-raw,width=640,height=480,format=RGB,framerate=30/1 ! \
  tensor_converter ! tensor_filter framework=tensorflow model=tf_model/ssdlite_mobilenet_v2.pb \
      input=3:640:480:1 inputname=image_tensor inputtype=uint8 \
      output=1:1:1:1,100:1:1:1,100:1:1:1,4:100:1:1 \
      outputname=num_detections,detection_classes,detection_scores,detection_boxes \
      outputtype=float32,float32,float32,float32 output-combination=i0,o0,o1,o2,o3 ! \
  tensor_demux name=demux tensorpick=0,1:2:3:4 demux.src_1 ! queue leaky=2 max-size-buffers=2 ! \
    tensor_decoder mode=bounding_boxes option1=mobilenet-ssd-postprocess option2=tf_model/coco_labels_list.txt option4=640:480 option5=640:480 ! \
    compositor name=mix sink_0::zorder=2 sink_1::zorder=1 ! videoconvert ! ximagesink \
  demux.src_0 ! queue leaky=2 max-size-buffers=2 ! tensor_decoder mode=direct_video ! videoconvert ! mix.
</code></pre>
<ul>
<li>Graphical description of the pipeline
<img src="gst/nnstreamer/tensor_filter/./filter_input_combi.png" alt="combi-pipeline-img" id="combipipelineimg">
</li>
</ul>
<h2 id="subcomponents">Sub-Components</h2>
<h3 id="main-tensor_filterc">Main <code>tensor_filter.c</code>
</h3>
<p>This is the main placeholder for all different subcomponents. With the property, <code>FRAMEWORK</code>, this main component loads the proper subcomponent (e.g., tensorflow-lite support, custom support, or other additional NNFW supports).<br>
The main component is supposed process the standard properties for subcomponents as well as processing the input/output dimensions.<br>
The subcomponents as supposed to fill in <code>GstTensor_Filter_Framework</code> struct and register it with <code>supported</code> array in <code>tensor_filter.h</code>.<br>
Note that the registering structure may be updated later. (We may follow what <code>Linux.kernel/drivers/devfreq/devfreq.c</code> does)</p>
<h3 id="tensorflowlite-support-tensor_filter_tensorflow_litecc">Tensorflow-lite support, <code>tensor_filter_tensorflow_lite.cc</code>
</h3>
<p>This should fill in <code>GstTensor_Filter_Framework</code> supporting tensorflow_lite.</p>
<h3 id="custom-function-support-tensor_filter_customc">Custom function support, <code>tensor_filter_custom.c</code>
</h3>
<p>Neural network and streameline developers may define their own tensor postprocessing operations with tensor_filter_custom.<br>
With <code>nnstreamer-devel</code> package installed at build time (e.g., <code>BuildRequires: pkgconfig(nnstreamer)</code> in .spec file), develerops can implement their own functions and expose their functions via <code>NNStreamer_custom_class</code> defined in <code>tensor_filter_custom.h</code>.<br>
The resulting custom developer plugin should exist as a shared library (.so) with the symbol NNStreamer_custom exposed with all the func defined in NNStreamer_custom_class.</p>

    </div>
        




		
	</div>
	<div id="search_results">
		<p>The results of the search are</p>
	</div>
	<div id="footer">
		    

	</div>
</div>

<div id="toc-column">
	
		<div class="edit-button">
		

	</div>
		<div id="toc-wrapper">
		<nav id="toc"></nav>
	</div>
</div>
</div>
</main>


<script src="assets/js/navbar_offset_scroller.js"></script>
</body>
</html>

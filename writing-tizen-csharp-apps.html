<!DOCTYPE html>
<html lang="en">
<head>

<base href=".">

<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>Writing Tizen C# apps</title>

<link rel="stylesheet" href="assets/css/custom_bootstrap.css" type="text/css">
<link rel="stylesheet" href="assets/css/bootstrap-toc.min.css" type="text/css">
<link rel="stylesheet" href="assets/css/frontend.css" type="text/css">
<link rel="stylesheet" href="assets/css/jquery.mCustomScrollbar.min.css">
<link rel="stylesheet" href="assets/js/search/enable_search.css" type="text/css">

<link rel="stylesheet" href="assets/css/extra_frontend.css" type="text/css">
<link rel="stylesheet" href="assets/css/prism-tomorrow.css" type="text/css">

<script src="assets/js/mustache.min.js"></script>
<script src="assets/js/jquery.js"></script>
<script src="assets/js/bootstrap.js"></script>
<script src="assets/js/scrollspy.js"></script>
<script src="assets/js/typeahead.jquery.min.js"></script>
<script src="assets/js/search.js"></script>
<script src="assets/js/compare-versions.js"></script>
<script src="assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
<script src="assets/js/bootstrap-toc.min.js"></script>
<script src="assets/js/jquery.touchSwipe.min.js"></script>
<script src="assets/js/anchor.min.js"></script>
<script src="assets/js/tag_filtering.js"></script>
<script src="assets/js/language_switching.js"></script>

<script src="assets/js/lines_around_headings.js"></script>

<script src="assets/js/prism-core.js"></script>
<script src="assets/js/prism-autoloader.js"></script>
<script src="assets/js/prism_autoloader_path_override.js"></script>
<script src="assets/js/trie.js"></script>

<link rel="icon" type="image/png" href="assets/images/nnstreamer_logo.png">

</head>

<body class="no-script
">

<script>
$('body').removeClass('no-script');
</script>

<nav class="navbar navbar-fixed-top navbar-default" id="topnav">
	<div class="container-fluid">
		<div class="navbar-right">
			<a id="toc-toggle">
				<span class="glyphicon glyphicon-menu-right"></span>
				<span class="glyphicon glyphicon-menu-left"></span>
			</a>
			<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-wrapper" aria-expanded="false">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			<form class="navbar-form pull-right" id="navbar-search-form">
                               <div class="form-group has-feedback">
                                       <input type="text" class="form-control input-sm" name="search" id="sidenav-lookup-field" placeholder="search" disabled>
				       <span class="glyphicon glyphicon-search form-control-feedback" id="search-mgn-glass"></span>
                               </div>
                        </form>
		</div>
		<div class="navbar-header">
			<a id="sidenav-toggle">
				<span class="glyphicon glyphicon-menu-right"></span>
				<span class="glyphicon glyphicon-menu-left"></span>
			</a>
			<a id="home-link" href="index.html" class="hotdoc-navbar-brand">
				<img src="assets/images/nnstreamer_logo.png" alt="Home">
			</a>
		</div>
		<div class="navbar-collapse collapse" id="navbar-wrapper">
			<ul class="nav navbar-nav" id="menu">
				
<li class="dropdown">
    <a class="dropdown-toggle" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
        API References<span class="caret"></span>
    </a>
	<ul class="dropdown-menu" id="modules-menu">
					<li>
				<a href="doc-index.html">NNStreamer doc</a>
			</li>
					<li>
				<a href="gst/nnstreamer/README.html">NNStreamer Elements</a>
			</li>
					<li>
				<a href="nnstreamer-example/index.html">NNStreamer Examples</a>
			</li>
					<li>
				<a href="API-reference.html">API reference</a>
			</li>
		</ul>
</li>

<li>
	<a href="doc-index.html">Documents</a>
</li>
<li>
	<a href="gst/nnstreamer/README.html">Elements</a>
</li>
<li>
	<a href="tutorials.html">Tutorials</a>
</li>
<li>
	<a href="API-reference.html">API reference</a>
</li>

			</ul>
			<div class="hidden-xs hidden-sm navbar-text navbar-center">
							</div>
		</div>
	</div>
</nav>

<main>
<div data-extension="core" data-hotdoc-in-toplevel="True" data-hotdoc-project="NNStreamer" data-hotdoc-ref="writing-tizen-csharp-apps.html" class="page_container" id="page-wrapper">
<script src="assets/js/utils.js"></script>

<div class="panel panel-collapse oc-collapsed" id="sidenav" data-hotdoc-role="navigation">
	<script src="assets/js/navigation.js"></script>
	<script src="assets/js/sitemap.js"></script>
</div>

<div id="body">
	<div id="main">
				    <div id="page-description" data-hotdoc-role="main">
        <h1 id="writing-a-tizen-net-application">Writing a Tizen .NET Application</h1>
<p>This document provides you how to write a Tizen .NET Application with Machine Learning APIs.
Since Tizen 5.5, Machine Learning Inference functionality has been provided on Mobile, Wearable and TV profile.</p>
<h2 id="installing-visual-studio-tools-for-tizen">Installing Visual Studio Tools for Tizen</h2>
<p>In order to use this functionality, you need to install Visual Studio Tools for Tizen and Tizen SDK. You can find the detailed guide for this at the below link.</p>
<ul>
<li>https://docs.tizen.org/application/vstools/install</li>
</ul>
<h2 id="machine-learning">Machine Learning</h2>
<p>Machine learning (ML) inference feature introduces how you can easily invoke the neural network model and get the inference output result effortlessly and efficiently.</p>
<p>You can use the following machine learning feature in your .NET applications:</p>
<p>You can use the <code>Tizen.MachineLearning.Inference.SingleShot</code> class, to load the existing neural network model or your own specific model from the storage. After loading the model, you can invoke it with a single instance of input data. Then, you can get the inference output result.</p>
<p>You can also use the <code>Pipeline</code> feature to manage the topology of data and the interconnection between processors and models. This feature is available in Native APIs from Tizen 5.5. However, this feature is not available in .NET APIs. This feature will be available in the .NET APIs from the next Tizen version.</p>
<p>The main features of the Tizen.MachineLearning.Inference are:</p>
<ul>
<li>
<p>Managing tensor information, which is the metadata: dimensions and types of tensors</p>
<p>You can <a href="writing-tizen-csharp-apps.html#manage">configure the input and output Tensor Information</a> such as its name, data type and dimension.</p>
</li>
<li>
<p>Loading a neural network model and configuring a runtime environment</p>
<p>You can <a href="writing-tizen-csharp-apps.html#load">load the neural network model from storage and configure a runtime environment</a>.</p>
</li>
<li>
<p>Invoking the neural network model with input data</p>
<p>After setting up the SingleShot instance with its required information, you can <a href="writing-tizen-csharp-apps.html#invoke">invoke the model with the input data and get the inference output result</a>.</p>
</li>
<li>
<p>Fetching the inference result after invoking</p>
<p>You can <a href="writing-tizen-csharp-apps.html#fetch">fetch the inference result</a> after invoking the respective model.</p>
</li>
</ul>
<h2 id="prerequisites">Prerequisites</h2>
<p>To enable your application to use the Machine Learning Inference API functionality:</p>
<ol>
<li>
<p>To use the methods and properties of the <code>Tizen.MachineLearning.Inference.SingleShot</code> class or its related classes such as <code>Tizen.MachineLearning.Inference.TensorsData</code> and <code>Tizen.MachineLearning.Inference.TensorsInfo</code>, include the <code>Tizen.MachineLearning.Inference</code> namespace in your application:</p>
<pre><code class="language-C#">using Tizen.MachineLearning.Inference;
</code></pre>
</li>
<li>
<p>If the model file you want to use is located in the <strong>media storage</strong> or the <strong>external storage</strong>, the application has to request permission by adding the following privileges to the <code>tizen-manifest.xml</code> file:</p>
<pre><code class="language-xml">&lt;privileges&gt;
  &lt;!--To access media storage--&gt;
  &lt;privilege&gt;http://tizen.org/privilege/mediastorage&lt;/privilege&gt;

  &lt;!--To access, read, and write to the external storage--&gt;
  &lt;privilege&gt;http://tizen.org/privilege/externalstorage&lt;/privilege&gt;
&lt;/privileges&gt;
</code></pre>
</li>
</ol>
<p><a name="manage"></a></p>
<h2 id="managing-tensor-information">Managing Tensor Information</h2>
<p>In the example mentioned in this page, the MobileNet v1 model for TensorFlow Lite is used. This model is used for image classification. The input data type of the model is specified as bit width of each Tensor and its input dimension is <code>3 X 224 X 224</code>. The output data type of the model is the same as the input datatype but the output dimension is <code>1001 X 1 X 1 X 1</code>.</p>
<p>To configure the tensor information, you need to create a new instance of the <code>Tizen.MachineLearning.Inference.TensorsInfo</code> class. Then, you can add the tensor information such as datatype, dimension, and name (optional) as shown in the following code:</p>
<pre><code class="language-C#">/* Input Dimension: 3 * 224 * 224 */
TensorsInfo in_info = new TensorsInfo();
in_info.AddTensorInfo(TensorType.UInt8, new int[4] { 3, 224, 224, 1 });

/* Output Dimension: 1001 for classification */
TensorsData out_info = new TensorsInfo();
out_info.AddTensorInfo(TensorType.UInt8, new int[4] { 1001, 1, 1, 1 });
</code></pre>
<p><a name="load"></a></p>
<h2 id="loading-neural-network-model-and-configuring-runtime-environment">Loading Neural Network Model and Configuring Runtime Environment</h2>
<ol>
<li>
<p>Since the model file is located in the resource directory of your own application, you need to get its absolute path:</p>
<pre><code class="language-C#">string ResourcePath = Tizen.Applications.Application.Current.DirectoryInfo.Resource;
string model_path = ResourcePath + "models/mobilenet_v1_1.0_224_quant.tflite";
</code></pre>
</li>
<li>
<p>You can load the neural network model from storage and configure a runtime environment with the <code>Tizen.MachineLearning.Inference.SingleShot</code> class. The first parameter is the absolute path to the neural network model file. The remaining two parameters are the input and the output <code>TensorsInfo</code> instances. If there is an invalid parameter, <code>ArgumentException</code> is raised:</p>
<pre><code class="language-C#">/* Create SingleShot instance with model information */
SingleShot single = new SingleShot(model_path, in_info, out_info);
</code></pre>
</li>
</ol>
<p><a name="invoke"></a></p>
<h2 id="invoking-neural-network-model-using-input-data">Invoking Neural Network Model using Input Data</h2>
<p>To invoke the neural network model, you need to create the <code>Tizen.MachineLearning.Inference.TensorsData</code> instance to pass the input data of the model. You can add various types of tensor data, which are already specified in the <code>TensorInfo</code> instance. However, the maximum size of <code>TensorsData</code> is 16. If the limit is exceeded, then <code>IndexOutOfRangeException</code> is raised. Input data is passed in a byte array format, byte[]:</p>
<pre><code class="language-C#">/* Input data for test */
byte[] in_buffer = new byte[3 * 224 * 224 * 1];

/* Set the input tensor data */
TensorsData in_data = in_info.GetTensorsData();
in_data.SetTensorData(0, in_buffer);
</code></pre>
<p>After preparing the input data, you can invoke the model and get the inference output result. The <code>SingleShot.Invoke()</code> method gets the input data to be inferred as a parameter and returns the <code>Tizen.MachineLearning.InferenceTensorsData</code> instance, which contains the inference result:</p>
<pre><code class="language-C#">/* Invoke the model and get the inference result */
TensorsData out_data = single.Invoke(in_data);
</code></pre>
<p><a name="fetch"></a></p>
<h2 id="fetching-inference-result">Fetching Inference Result</h2>
<p>After calling the <code>Invoke()</code> method of the <code>Tizen.MachineLearning.Inference.SingleShot</code> class,
the <code>Tizen.MachineLearning.Inference.TensorsData</code> instance is returned as the inference result.
The result can have multiple output data. Therefore, you have to fetch each data using the <code>GetTensorData()</code> method. If the limit is exceeded, then <code>IndexOutOfRangeException</code> is raised:</p>
<pre><code class="language-C#">/* Get the first Tensor data from the inference result */
byte[] out_buffer = out_data.GetTensorData(0);
</code></pre>
<p>The <code>TensorsData</code> class is used to send the input data to a neural network model. In addition, it provides the <code>Count</code> property to get the number of tensors:</p>
<pre><code class="language-C#">/* Get the number of Tensor in TensorsData instance */
var count = out_data.Count;
</code></pre>
<h2 id="related-information">Related Information</h2>
<ul>
<li>Dependencies
<ul>
<li>Tizen 5.5 and Higher</li>
</ul>
</li>
</ul>

    </div>
        




		
	</div>
	<div id="search_results">
		<p>The results of the search are</p>
	</div>
	<div id="footer">
		    

	</div>
</div>

<div id="toc-column">
	
		<div class="edit-button">
		

	</div>
		<div id="toc-wrapper">
		<nav id="toc"></nav>
	</div>
</div>
</div>
</main>


<script src="assets/js/navbar_offset_scroller.js"></script>
</body>
</html>
